import requests
import json
import time
import re
from geopy.geocoders import Nominatim
from bs4 import BeautifulSoup

# üìå Lista de fuentes de noticias (actualizada con medios nacionales y locales)
FUENTES = [
    "https://www.lanacion.com.ar",
    "https://www.pagina12.com.ar",
    "https://www.ellitoral.com",
    "https://www.infobae.com",
    "https://www.perfil.com",
    "https://www.clarin.com",
    "https://tn.com.ar",
    "https://www.ambito.com",
    "https://www.argentinamunicipal.com.ar",
    "https://www.diarionecochea.com",
    "https://www.eldiarioar.com",
    "https://www.lavoz.com.ar",
    "https://www.tiempoar.com.ar",
    "https://www.lacapital.com.ar",
    "https://www.elonce.com",
    "https://www.chacodiapordia.com",
    "https://www.mdzol.com",
    "https://www.misionesonline.net",
    "https://www.eltribuno.com",
    "https://www.unoentrerios.com.ar",
    "https://www.lagaceta.com.ar",
    "https://www.losandes.com.ar",
    "https://www.elcomercial.com.ar",
    "https://www.cadena3.com",
    "https://www.era-verde.com.ar",
    "https://www.elpopular.com.ar",
    "https://www.laverdadonline.com",
    "https://www.noticiaslasflores.com.ar",
    "https://www.realpolitik.com.ar",
    "https://www.bichosdecampo.com",
    "https://www.laizquierdadiario.com",
    "https://www.eldiarionorte.com.ar",
    "https://www.ruralaldia.com.ar",
    "https://www.agrositio.com.ar",
    "https://www.elmensajerodiario.com.ar",
    "https://www.lavozdelpueblo.com.ar",
    "https://www.entrelineas.info",
    "https://www.quedigital.com.ar",
    "https://www.elecodetandil.com.ar",
    "https://www.lanueva.com",
    "https://www.infocielo.com",
    "https://elespectador.com",
    "https://www.eldiariocba.com.ar",
    "https://www.memo.com.ar",
    "https://www.rionegro.com.ar",
    "https://www.elliberal.com.ar",
    "https://www.laarena.com.ar",
    "https://www.tiempodesanjuan.com",
    "https://www.elciudadanoweb.com",
    "https://www.puntal.com.ar",
    "https://www.fundeps.org",
    "https://www.eltrdebaldia.com",
    "https://www.mercedesya.com",
    "https://www.bragadoinforma.com.ar",
    "https://www.canalabierto.com.ar",
    "https://www.periodicoimpacto.com.ar",
    "https://www.elbalcarce.com",
    "https://www.eltime.com.ar",
    "https://www.realpolitik.com.ar",
    "https://www.eltiempo.com.ar",
    "https://www.abcdehoy.com.ar",
    "https://www.diariolaverdad.com.ar",
    "https://www.elcastellidigital.com.ar",
    "https://www.ramonactualidad.com.ar",
    "https://www.elemental.com.ar",
    "https://www.agrositio.com.ar",
    "https://www.noticiasguido.com.ar"
]

# üìå Configuraci√≥n del geolocalizador
geolocalizador = Nominatim(user_agent="geo_scraper")

# üìå Funci√≥n para obtener coordenadas
def obtener_coordenadas(ubicacion):
    """Convierte una ciudad/localidad en coordenadas (lat, lon)"""
    try:
        ubicacion_info = geolocalizador.geocode(f"{ubicacion}, Argentina")
        if ubicacion_info:
            return [ubicacion_info.longitude, ubicacion_info.latitude]
    except:
        return None
    return None

# üìå Funci√≥n para buscar enlaces en un portal de noticias
def obtener_enlaces_de_busqueda(url_base):
    """Extrae enlaces de art√≠culos desde la p√°gina principal del medio"""
    try:
        response = requests.get(url_base, headers={'User-Agent': 'Mozilla/5.0'})
        if response.status_code != 200:
            print(f"‚ö†Ô∏è No se pudo acceder a: {url_base}")
            return []

        sopa = BeautifulSoup(response.text, "html.parser")
        enlaces = []

        for link in sopa.find_all("a", href=True):
            href = link["href"]
            if any(tema in href.lower() for tema in ["agroquimico", "pesticida", "contaminacion", "fumigacion"]):
                if href.startswith("http"):
                    enlaces.append(href)
                else:
                    enlaces.append(url_base + href)

        print(f"üîó Se encontraron {len(enlaces)} art√≠culos en {url_base}")
        return enlaces[:5]  # Limitar para evitar demasiadas solicitudes

    except Exception as e:
        print(f"‚ùå Error al obtener enlaces de {url_base}: {e}")
        return []

# üìå Funci√≥n para extraer datos de una noticia
def extraer_datos_noticia(url):
    """Extrae datos clave de una noticia"""
    try:
        response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})
        if response.status_code != 200:
            print(f"‚ö†Ô∏è No se pudo obtener la noticia: {url}")
            return None

        sopa = BeautifulSoup(response.text, "html.parser")
        texto = sopa.get_text().lower()

        # üìå Extraer t√≠tulo
        titulo = sopa.title.string.strip() if sopa.title else "T√≠tulo desconocido"

        # üìå Extraer fecha (solo a√±o)
        fecha = "Desconocida"
        for palabra in texto.split():
            if palabra.isdigit() and len(palabra) == 4:
                fecha = palabra
                break

        # üìå Extraer ubicaci√≥n
        ubicacion = None
        for palabra in texto.split():
            if palabra in ["buenos", "santa", "cordoba", "mendoza", "chaco", "misiones"]:
                ubicacion = palabra.capitalize() + ", Argentina"
                break

        coordenadas = obtener_coordenadas(ubicacion) if ubicacion else None

        # üìå Buscar menci√≥n a agroqu√≠micos
        menciona_agua = any(agua in texto for agua in ["r√≠o", "laguna", "napas", "agua", "contaminaci√≥n h√≠drica"])
        menciona_protesta = any(palabra in texto for palabra in ["denuncia", "protesta", "marcha", "juicio", "vecinos", "movilizaci√≥n"])

        # üìå Estructurar la noticia
        noticia = {
            "conflicto": titulo,
            "url": url,
            "fecha": fecha,
            "fuente": url.split("/")[2],
            "ubicacion": ubicacion if ubicacion else "Desconocida",
            "coordenadas": coordenadas if coordenadas else [0, 0],
            "agua": "S√≠" if menciona_agua else "No",
            "protestas": "S√≠" if menciona_protesta else "No"
        }

        print(f"‚úÖ Noticia extra√≠da: {noticia}")
        return noticia

    except Exception as e:
        print(f"‚ùå Error al extraer la noticia: {url} - {e}")
        return None

# üìå Extraer noticias de todas las fuentes
nuevas_noticias = []
for fuente in FUENTES:
    enlaces_noticias = obtener_enlaces_de_busqueda(fuente)
    for enlace in enlaces_noticias:
        noticia = extraer_datos_noticia(enlace)
        if noticia:
            nuevas_noticias.append(noticia)

# üìå Guardar en GeoJSON
with open("ConflictosGeorref_final_DEF.geojson", "w", encoding="utf-8") as f:
    json.dump(nuevas_noticias, f, indent=4, ensure_ascii=False)

print("‚úÖ Base de datos actualizada con nuevas noticias.")
